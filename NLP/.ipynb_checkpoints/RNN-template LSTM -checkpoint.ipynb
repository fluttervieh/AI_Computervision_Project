{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6548f38e",
   "metadata": {},
   "source": [
    "### Template for NLP project\n",
    "\n",
    "The aim of the project is to achieve the following:\n",
    " - Train a neural network that is **at least better than random guessing** on your dataset. The template contains the IMDB dataset for sentiment analysis, however, you can choose any other language related data set with the appropriate NLP task.\n",
    " - Investigate different neural network architectures (different hyperparameters, different layers, different pre-processing). Explain in the presentation, why the final network was selected! **Do not rely on black-box mechanisms.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c548b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow modules\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LayerNormalization, LSTM, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "# if you have installed a different version, replace 'r2.6'  with your version in links provided below\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b175c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n",
      "---review---\n",
      "[1, 307, 5, 1301, 20, 1026, 2511, 87, 2775, 52, 116, 5, 31, 7, 4, 91, 1220, 102, 13, 28, 110, 11, 6, 137, 13, 115, 219, 141, 35, 221, 956, 54, 13, 16, 11, 2714, 61, 322, 423, 12, 38, 76, 59, 1803, 72, 8, 2, 23, 5, 967, 12, 38, 85, 62, 358, 99]\n",
      "[1, 518, 12, 304, 6, 22, 231, 1300, 40, 2, 8, 721, 15, 1727, 117, 142, 15, 955, 2, 5, 2, 2, 15, 2, 6, 87, 20, 42, 6, 87, 229, 83, 6, 991, 31, 18, 4, 2088, 10, 10, 45, 24, 43, 15, 1660, 4669, 65, 47, 195, 3549, 5, 2, 231, 12, 878, 18, 60, 4, 91, 2, 7, 907, 8, 717, 2304, 60, 711, 309, 161, 2396, 38, 78, 45, 89, 2, 2, 4669, 2518, 89, 29, 2, 4, 1511, 83, 268, 58, 15, 2, 4, 3537, 199, 6, 1114, 2, 5, 6, 2, 2, 11, 940, 10, 10, 2, 717, 2, 136, 9, 17, 633, 1307, 4, 20, 4608, 19, 6, 2, 2455, 4764, 1062, 60, 151, 45, 1082, 702, 885, 2699, 1993, 5, 12, 2, 33, 57, 329, 74, 2, 234, 4, 370, 2, 143, 4, 2, 2, 7, 4, 4909, 1455, 40, 12, 9, 49, 243, 7, 2, 2, 2, 18, 4665, 2, 2, 665, 2, 4, 2, 2, 2, 11, 32, 68, 2, 225, 6, 2, 5, 2, 1483, 11, 89, 2, 2, 834, 15, 1791, 72, 55, 76, 7, 89, 860, 907, 952, 11, 4, 4997, 17, 48, 51, 9, 2399, 9, 2, 8, 89, 32, 4, 275, 791, 26, 2, 1004, 4, 2122, 10, 10, 813, 1218, 407, 759, 46, 4, 86, 324, 7, 2, 2, 4736, 125, 268, 39, 4, 2, 7, 6, 1871, 40, 2, 7, 4740, 582, 39, 49, 85, 1873, 42, 1055, 34, 4, 2, 410, 17, 151, 12, 9, 35, 2, 7, 4740, 32, 34, 410, 303, 23, 2, 8, 30, 164, 346, 7, 43, 15, 6, 767, 1872, 2, 143, 4, 2, 2, 11, 550, 1270, 718, 3346, 4, 1312, 19, 4, 2, 4, 370, 728, 367, 19, 90, 2, 4, 206, 17, 12, 571, 305, 7, 3712, 2, 17, 151, 45, 4, 370, 2, 143, 4, 1312, 5, 24, 4, 2, 4, 3282, 702, 2, 11, 4, 2, 393, 2, 178, 1004, 27, 419, 159, 75, 67, 4, 107, 351, 539, 39, 27, 2, 5, 7, 265, 4, 3868, 136, 10, 10, 76, 47, 77, 301, 7, 718, 2, 2, 4792, 27, 1168, 9, 24, 1097, 1112, 88, 73, 240, 718, 4462, 4, 232, 272, 320, 1168, 553, 395, 1168, 505, 90, 83, 35, 3694, 2, 7, 309, 4708, 2, 23, 4, 85, 508, 9, 31, 7, 4, 91, 1581, 973, 2837, 2, 126, 69, 582, 39, 6, 2, 7, 777, 354, 18, 670, 2, 11, 4, 4039, 342, 372, 2, 40, 178, 2, 59, 961, 8, 41, 109, 4, 208, 4655, 7, 2, 5, 921, 2, 6, 1307, 5, 55, 2184, 524]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "['the', 'version', 'to', 'date', 'on', 'list', 'draw', 'him', 'critical', 'very', 'love', 'to', 'by', 'br', 'of', 'its', 'tony', 'characters', 'was', 'one', 'life', 'this', 'is', 'go', 'was', 'best', 'least', 'should', 'so', 'done', 'result', 'no', 'was', 'with', 'this', 'understood', 'only', 'war', \"couldn't\", 'that', 'her', 'get', 'would', 'johnny', 'we', 'in', 'and', 'are', 'to', 'business', 'that', 'her', 'because', 'story', 'use', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# load imdb dataset\n",
    "# links to dataset\n",
    "# original dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# version in tensorflow: https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/imdb\n",
    "\n",
    "# select your vocabulary size\n",
    "vocabularySize = 5000\n",
    "# load data (it is already pre-processed)\n",
    "# optional: add other pre.processing steps like stopword removal\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=vocabularySize)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(xTrain), len(xTest)))\n",
    "\n",
    "# look at the data\n",
    "print('---review---')\n",
    "print(xTrain[123])\n",
    "print(xTrain[124])\n",
    "print('---label---')\n",
    "print(yTrain[123])\n",
    "\n",
    "# look at the respective words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in xTrain[123]])\n",
    "\n",
    "\n",
    "# other related dataset already in tensorflow:  reuters newswire classification dataset\n",
    "# see https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4358fc0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61883a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', \"shouldn't\", 'through', 'to', \"weren't\", 'these', 'at', 'just', 'wasn', \"doesn't\", 'didn', 'needn', 's', \"haven't\", 'between', 'it', 'yourself', 'because', 'too', 'having', 'ourselves', 'but', 'into', 'd', 'doing', 'her', 'or', 'over', \"don't\", 'doesn', 'which', 'until', 'are', 'down', \"you've\", 'other', \"isn't\", 'they', 'our', 'as', 'hasn', 'only', 'you', 'i', 'how', 'aren', 'ours', 'few', 'mightn', 'all', 'them', 'did', 'has', \"couldn't\", 'out', 'him', 'up', \"needn't\", 'yours', 'were', 'couldn', 'such', \"you're\", \"aren't\", 'do', 'y', 'its', 'while', 'me', \"you'd\", \"hadn't\", \"mightn't\", 'was', 'can', 'now', 'under', 'been', 'about', 'should', 'their', 'same', 'not', 'off', 'any', 'be', \"should've\", 'very', \"hasn't\", 'who', 'each', 'some', 'does', 'what', 'ma', 'with', 'in', 'won', \"you'll\", 'after', 'nor', 'then', 'against', 't', \"mustn't\", \"won't\", 'isn', 'my', 'the', 'further', 'will', 'whom', 'll', 'm', 'by', \"didn't\", 'wouldn', 'more', 'itself', 'yourselves', 'those', 'below', 'have', 'shan', 'most', \"wasn't\", 'hers', 'himself', 'being', 'why', \"shan't\", 'from', 'shouldn', 'herself', 'so', 'she', 'before', 'no', 'your', 'a', 'own', 'once', 'of', 'again', 'when', 'haven', 'weren', 'mustn', 're', 'we', 'don', 'there', \"she's\", \"it's\", 've', 'he', 'ain', 'this', 'an', \"wouldn't\", 'myself', 'here', 'than', 'his', 'am', 'theirs', 'where', 'o', 'both', 'for', 'during', 'above', 'if', 'on', 'and', 'is', 'had', 'hadn', 'that', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "#load Stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14fef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[530, 1613, 140, 5, 1170, 131, 30, 40, 29877, 149, 15496, 0, 587, 771, 197, 9, 621, 85, 96, 257, 3144, 18, 80, 1092, 396, 38, 39, 117, 89, 20830, 60, 363, 23, 177, 871, 82, 215, 33, 260, 14, 41026, 61, 22, 10, 86, 0, 11292, 168, 0, 29, 95, 119, 44, 423, 43, 87, 53, 12421, 6444, 68, 26232, 138, 332, 710, 78, 5132, 91, 134, 69, 1387, 1866, 52778, 13, 67, 147, 464, 74, 41, 141, 65, 169, 21, 122, 98, 27, 5460, 52, 1478, 34, 254, 46, 124, 48, 8634, 16, 8, 1196, 487, 100, 882, 92, 426, 827, 24007, 525, 20781, 58, 1, 1034, 77, 934, 8725, 1980, 31, 158, 39964, 50, 407, 9888, 145, 1905, 25, 41501, 88, 283, 6139, 306, 109, 135, 24086, 36, 0, 762, 35, 56, 156, 54, 126, 3, 202, 277, 4, 171, 51, 19932, 0, 0, 793, 72, 1558, 47, 439, 42, 13340, 26, 0, 11, 32, 583, 543, 130, 71, 24, 241, 9127, 118, 1601, 196, 15, 312, 749, 45, 20, 2, 6, 66, 0, 12, 9540]\n"
     ]
    }
   ],
   "source": [
    "stopWordNumbers = []\n",
    "\n",
    "for word in stopWords:\n",
    "    stopwordId = word2id.get(word, 0)\n",
    "    stopWordNumbers.append(stopwordId)\n",
    "\n",
    "print(stopWordNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eac8a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(xTrain))\n",
    "print(len(xTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75452f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove Words\n",
      "Step start: 1000 , End: 1999 , RemovedWords:  88135\n",
      "Step start: 2000 , End: 2999 , RemovedWords:  85981\n",
      "Step start: 3000 , End: 3999 , RemovedWords:  89561\n",
      "Step start: 4000 , End: 4999 , RemovedWords:  90407\n",
      "Step start: 5000 , End: 5999 , RemovedWords:  85889\n",
      "Step start: 6000 , End: 6999 , RemovedWords:  83455\n",
      "Step start: 7000 , End: 7999 , RemovedWords:  84079\n",
      "Step start: 8000 , End: 8999 , RemovedWords:  86079\n",
      "Step start: 9000 , End: 9999 , RemovedWords:  82952\n",
      "Step start: 10000 , End: 10999 , RemovedWords:  85457\n",
      "Step start: 11000 , End: 11999 , RemovedWords:  83881\n",
      "Step start: 12000 , End: 12999 , RemovedWords:  83455\n",
      "Step start: 13000 , End: 13999 , RemovedWords:  85520\n",
      "Step start: 14000 , End: 14999 , RemovedWords:  89032\n",
      "Step start: 15000 , End: 15999 , RemovedWords:  88430\n",
      "Step start: 16000 , End: 16999 , RemovedWords:  84654\n",
      "Step start: 17000 , End: 17999 , RemovedWords:  85100\n",
      "Step start: 18000 , End: 18999 , RemovedWords:  85459\n",
      "Step start: 19000 , End: 19999 , RemovedWords:  86463\n",
      "Step start: 20000 , End: 20999 , RemovedWords:  85233\n",
      "Step start: 21000 , End: 21999 , RemovedWords:  87870\n",
      "Step start: 22000 , End: 22999 , RemovedWords:  84085\n",
      "Step start: 23000 , End: 23999 , RemovedWords:  87243\n",
      "Step start: 24000 , End: 24999 , RemovedWords:  87700\n",
      "Step start: 25000 , End: 25999 , RemovedWords:  87650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for review in xTrain:\n",
    "def removeStopWordsSpace(minValue, maxValue):\n",
    "    removedWords = 0\n",
    "    counter = minValue\n",
    "\n",
    "    while counter <= maxValue:\n",
    "        #print(\"StartLen:\",len(xTrain[counter]))\n",
    "        for word in xTrain[counter]:\n",
    "            if word in stopWordNumbers:\n",
    "                xTrain[counter].remove(word)\n",
    "                removedWords+=1\n",
    "        #print(\"EndLine:\", len(xTrain[counter]))\n",
    "        counter+=1\n",
    "    return removedWords\n",
    "\n",
    "def remvoeStopWords():\n",
    "    print(\"Remove Words\")\n",
    "    startSpace = 0\n",
    "    endSpace = 999\n",
    "    while endSpace <= 25000:\n",
    "        removedWords = removeStopWordsSpace(startSpace, endSpace)\n",
    "        startSpace+=1000\n",
    "        endSpace+=1000\n",
    "        print(\"Step start:\", startSpace, \", End:\", endSpace, \", RemovedWords: \", removedWords)\n",
    "\n",
    "remvoeStopWords()\n",
    "\n",
    "\n",
    "#print(\"xTrain: len: \", len(xTrain[123], \", words:\", xTrain[123])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd8c8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.api._v2.keras.datasets.imdb' from 'C:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\keras\\\\api\\\\_v2\\\\keras\\\\datasets\\\\imdb\\\\__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7933da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum train review length: 1566\n",
      "Maximum test review length: 2315\n",
      "Minimum train review length: 6\n",
      "Minimum test review length: 7\n"
     ]
    }
   ],
   "source": [
    "# get properties of the dataset\n",
    "print('Maximum train review length: {}'.format(len(max(xTrain, key=len))))\n",
    "print('Maximum test review length: {}'.format(len(max(xTest, key=len))))\n",
    "print('Minimum train review length: {}'.format(len(min(xTrain, key=len))))\n",
    "print('Minimum test review length: {}'.format(len(min(xTest, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select maximum number of words as input lengt\n",
    "# pad or truncated (this is done automatically) your data\n",
    "maxWords = 1000\n",
    "xTrain = sequence.pad_sequences(xTrain, maxlen=maxWords)\n",
    "xTest = sequence.pad_sequences(xTest, maxlen=maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45999cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 16)          80000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1000, 128)         74240     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000, 128)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 289,985\n",
      "Trainable params: 289,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # setup the neural network architecture\n",
    "# # check out the respective tensorflow help page: https://www.tensorflow.org/guide/keras/rnn\n",
    "# model=Sequential()\n",
    "\n",
    "# # define size of embedding, see https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Embedding\n",
    "# # optional: use a different embedding like word2vec or other options available within tensorflow \n",
    "# embeddingSize = 16\n",
    "# model.add(Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "# # add recurrent layers: \n",
    "# # e.g. a SimpleRNN (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/SimpleRNN) with\n",
    "# # LayerNormalization (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "# model.add(SimpleRNN(100))\n",
    "# model.add(LayerNormalization())\n",
    "\n",
    "# # add layer for output\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # print model and check number of parameters\n",
    "# print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "embedding_vector_features=45\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "\n",
    "embeddingSize = 16\n",
    "model.add(Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "model.add(LSTM(128,input_shape=(xTrain.shape),activation='relu',return_sequences=True))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# for units in [128,128,64,32]:\n",
    "\n",
    "# model.add(Dense(units,activation='relu'))\n",
    "\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1 ,activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46bb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c6f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  2/390 [..............................] - ETA: 1:08:10 - loss: 0.6930 - accuracy: 0.5547"
     ]
    }
   ],
   "source": [
    "# set parameters for network training\n",
    "batchSize = 64\n",
    "numEpochs = 5\n",
    "\n",
    "# train your model\n",
    "model.compile(loss='binary_crossentropy',  optimizer=opt, metrics=['accuracy'])\n",
    "xValid, yValid = xTrain[:batchSize], yTrain[:batchSize]\n",
    "xTrain2, yTrain2 = xTrain[batchSize:], yTrain[batchSize:]\n",
    "hist = model.fit(xTrain2, yTrain2, validation_data=(xValid, yValid), batch_size=batchSize, epochs=numEpochs)\n",
    "\n",
    "# check result\n",
    "scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856382c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
