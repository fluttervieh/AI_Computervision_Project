{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6548f38e",
   "metadata": {},
   "source": [
    "### Template for NLP project\n",
    "\n",
    "The aim of the project is to achieve the following:\n",
    " - Train a neural network that is **at least better than random guessing** on your dataset. The template contains the IMDB dataset for sentiment analysis, however, you can choose any other language related data set with the appropriate NLP task.\n",
    " - Investigate different neural network architectures (different hyperparameters, different layers, different pre-processing). Explain in the presentation, why the final network was selected! **Do not rely on black-box mechanisms.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c548b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow modules\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LayerNormalization\n",
    "import tensorflow\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# if you have installed a different version, replace 'r2.6'  with your version in links provided below\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b175c046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n",
      "---review---\n",
      "[1, 307, 5, 1301, 20, 1026, 2511, 87, 2775, 52, 116, 5, 31, 7, 4, 91, 1220, 102, 13, 28, 110, 11, 6, 137, 13, 115, 219, 141, 35, 221, 956, 54, 13, 16, 11, 2714, 61, 322, 423, 12, 38, 76, 59, 1803, 72, 8, 2, 23, 5, 967, 12, 38, 85, 62, 358, 99]\n",
      "[1, 518, 12, 304, 6, 22, 231, 1300, 40, 2, 8, 721, 15, 1727, 117, 142, 15, 955, 2, 5, 2, 2, 15, 2, 6, 87, 20, 42, 6, 87, 229, 83, 6, 991, 31, 18, 4, 2088, 10, 10, 45, 24, 43, 15, 1660, 4669, 65, 47, 195, 3549, 5, 2, 231, 12, 878, 18, 60, 4, 91, 2, 7, 907, 8, 717, 2304, 60, 711, 309, 161, 2396, 38, 78, 45, 89, 2, 2, 4669, 2518, 89, 29, 2, 4, 1511, 83, 268, 58, 15, 2, 4, 3537, 199, 6, 1114, 2, 5, 6, 2, 2, 11, 940, 10, 10, 2, 717, 2, 136, 9, 17, 633, 1307, 4, 20, 4608, 19, 6, 2, 2455, 4764, 1062, 60, 151, 45, 1082, 702, 885, 2699, 1993, 5, 12, 2, 33, 57, 329, 74, 2, 234, 4, 370, 2, 143, 4, 2, 2, 7, 4, 4909, 1455, 40, 12, 9, 49, 243, 7, 2, 2, 2, 18, 4665, 2, 2, 665, 2, 4, 2, 2, 2, 11, 32, 68, 2, 225, 6, 2, 5, 2, 1483, 11, 89, 2, 2, 834, 15, 1791, 72, 55, 76, 7, 89, 860, 907, 952, 11, 4, 4997, 17, 48, 51, 9, 2399, 9, 2, 8, 89, 32, 4, 275, 791, 26, 2, 1004, 4, 2122, 10, 10, 813, 1218, 407, 759, 46, 4, 86, 324, 7, 2, 2, 4736, 125, 268, 39, 4, 2, 7, 6, 1871, 40, 2, 7, 4740, 582, 39, 49, 85, 1873, 42, 1055, 34, 4, 2, 410, 17, 151, 12, 9, 35, 2, 7, 4740, 32, 34, 410, 303, 23, 2, 8, 30, 164, 346, 7, 43, 15, 6, 767, 1872, 2, 143, 4, 2, 2, 11, 550, 1270, 718, 3346, 4, 1312, 19, 4, 2, 4, 370, 728, 367, 19, 90, 2, 4, 206, 17, 12, 571, 305, 7, 3712, 2, 17, 151, 45, 4, 370, 2, 143, 4, 1312, 5, 24, 4, 2, 4, 3282, 702, 2, 11, 4, 2, 393, 2, 178, 1004, 27, 419, 159, 75, 67, 4, 107, 351, 539, 39, 27, 2, 5, 7, 265, 4, 3868, 136, 10, 10, 76, 47, 77, 301, 7, 718, 2, 2, 4792, 27, 1168, 9, 24, 1097, 1112, 88, 73, 240, 718, 4462, 4, 232, 272, 320, 1168, 553, 395, 1168, 505, 90, 83, 35, 3694, 2, 7, 309, 4708, 2, 23, 4, 85, 508, 9, 31, 7, 4, 91, 1581, 973, 2837, 2, 126, 69, 582, 39, 6, 2, 7, 777, 354, 18, 670, 2, 11, 4, 4039, 342, 372, 2, 40, 178, 2, 59, 961, 8, 41, 109, 4, 208, 4655, 7, 2, 5, 921, 2, 6, 1307, 5, 55, 2184, 524]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "['the', 'version', 'to', 'date', 'on', 'list', 'draw', 'him', 'critical', 'very', 'love', 'to', 'by', 'br', 'of', 'its', 'tony', 'characters', 'was', 'one', 'life', 'this', 'is', 'go', 'was', 'best', 'least', 'should', 'so', 'done', 'result', 'no', 'was', 'with', 'this', 'understood', 'only', 'war', \"couldn't\", 'that', 'her', 'get', 'would', 'johnny', 'we', 'in', 'and', 'are', 'to', 'business', 'that', 'her', 'because', 'story', 'use', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# load imdb dataset\n",
    "# links to dataset\n",
    "# original dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# version in tensorflow: https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/imdb\n",
    "\n",
    "# select your vocabulary size\n",
    "vocabularySize = 5000\n",
    "# load data (it is already pre-processed)\n",
    "# optional: add other pre.processing steps like stopword removal\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=vocabularySize)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(xTrain), len(xTest)))\n",
    "\n",
    "# look at the data\n",
    "print('---review---')\n",
    "print(xTrain[123])\n",
    "print(xTrain[124])\n",
    "print('---label---')\n",
    "print(yTrain[123])\n",
    "\n",
    "# look at the respective words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in xTrain[123]])\n",
    "\n",
    "\n",
    "# other related dataset already in tensorflow:  reuters newswire classification dataset\n",
    "# see https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4358fc0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61883a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', \"it's\", 'be', 'hers', 'myself', 'while', 'between', \"weren't\", 'had', 'hadn', \"doesn't\", \"don't\", 'wasn', 'some', 'ourselves', 'about', 'on', 'nor', 'or', 'yourselves', 'through', 'few', \"won't\", 'am', \"hadn't\", \"mightn't\", 'then', 'any', 'down', 'don', 's', 'both', 'above', \"wouldn't\", 'into', 'having', 'yourself', 'from', 'before', 'being', 'm', 'who', 'been', 'herself', \"couldn't\", 'below', 'there', 'over', 'shouldn', 'did', 'can', 'your', 'with', 'mightn', 'where', \"wasn't\", 'out', 'very', 'ma', \"hasn't\", 'shan', 'd', 'needn', 'too', 'because', 'up', 'his', 'couldn', 'all', 'didn', 'hasn', 'whom', 'haven', 'we', 'against', 'further', 'me', 'own', 've', 'should', 'is', \"you're\", 'that', 'off', 'aren', 'and', 'those', 'not', 'during', 'by', 'y', 'as', 'was', \"needn't\", 'only', 'ours', \"she's\", 'an', 'himself', 'here', 'her', 'such', 'has', 'itself', 'of', \"shan't\", 'which', 'you', \"you've\", 'were', \"didn't\", 'wouldn', 'same', 'the', 'a', \"you'd\", 'in', \"you'll\", 'ain', 'now', 'why', \"aren't\", \"isn't\", 'but', 'their', 'our', 'theirs', 'isn', 'it', \"should've\", 'again', 'these', 'each', 'than', 'have', 'most', 'this', 'so', \"shouldn't\", 'once', 'does', 'mustn', 'them', 'what', 'how', 'my', 'll', \"haven't\", 'themselves', 'if', 'yours', 't', 'she', 're', 'its', 'do', 'no', 'will', 'are', \"mustn't\", 'after', 'doesn', 'weren', 'he', 'for', 'under', 'when', 'him', 'at', 'other', 'o', 'to', 'doing', \"that'll\", 'until', 'they', 'just', 'won', 'more'}\n"
     ]
    }
   ],
   "source": [
    "#load Stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14fef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 42, 27, 6139, 543, 134, 197, 1170, 66, 0, 149, 89, 29877, 46, 3144, 41, 20, 882, 39, 9888, 140, 168, 525, 241, 1866, 52778, 92, 98, 177, 1558, 587, 196, 749, 583, 80, 257, 621, 36, 156, 109, 1980, 34, 74, 762, 423, 1905, 47, 117, 0, 119, 67, 126, 16, 0, 118, 283, 43, 52, 8634, 1478, 41501, 1092, 0, 96, 85, 53, 24, 26232, 29, 15496, 41026, 934, 19932, 72, 426, 1034, 69, 202, 13340, 141, 6, 332, 12, 122, 0, 2, 145, 21, 312, 31, 5132, 14, 13, 12421, 61, 11292, 439, 32, 306, 130, 38, 138, 44, 407, 4, 24086, 60, 22, 871, 68, 158, 39964, 169, 1, 3, 1387, 8, 487, 0, 147, 135, 710, 215, 18, 65, 260, 9127, 20781, 9, 5460, 171, 131, 254, 71, 25, 88, 11, 35, 1613, 277, 124, 0, 95, 48, 86, 58, 8725, 771, 530, 45, 6444, 827, 56, 793, 91, 78, 54, 77, 23, 24007, 100, 20830, 0, 26, 15, 464, 51, 87, 30, 82, 1601, 5, 396, 9540, 363, 33, 40, 1196, 50]\n"
     ]
    }
   ],
   "source": [
    "stopWordNumbers = []\n",
    "\n",
    "for word in stopWords:\n",
    "    stopwordId = word2id.get(word, 0)\n",
    "    stopWordNumbers.append(stopwordId)\n",
    "\n",
    "print(stopWordNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd0e6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(xTrain))\n",
    "print(len(xTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c036c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartLen: 132\n",
      "EndLine: 93\n",
      "StartLen: 116\n",
      "EndLine: 91\n",
      "StartLen: 89\n",
      "EndLine: 67\n",
      "StartLen: 351\n",
      "EndLine: 278\n",
      "StartLen: 96\n",
      "EndLine: 78\n",
      "StartLen: 28\n",
      "EndLine: 19\n",
      "StartLen: 78\n",
      "EndLine: 60\n",
      "StartLen: 361\n",
      "EndLine: 285\n",
      "StartLen: 147\n",
      "EndLine: 111\n",
      "StartLen: 85\n",
      "EndLine: 66\n",
      "StartLen: 300\n",
      "EndLine: 236\n",
      "StartLen: 62\n",
      "EndLine: 42\n",
      "StartLen: 71\n",
      "EndLine: 51\n",
      "StartLen: 155\n",
      "EndLine: 125\n",
      "StartLen: 67\n",
      "EndLine: 52\n",
      "StartLen: 84\n",
      "EndLine: 65\n",
      "StartLen: 102\n",
      "EndLine: 73\n",
      "StartLen: 476\n",
      "EndLine: 367\n",
      "StartLen: 138\n",
      "EndLine: 110\n",
      "StartLen: 111\n",
      "EndLine: 84\n",
      "StartLen: 75\n",
      "EndLine: 54\n",
      "StartLen: 86\n",
      "EndLine: 63\n",
      "StartLen: 165\n",
      "EndLine: 127\n",
      "StartLen: 566\n",
      "EndLine: 427\n",
      "StartLen: 58\n",
      "EndLine: 45\n",
      "StartLen: 93\n",
      "EndLine: 72\n",
      "StartLen: 139\n",
      "EndLine: 107\n",
      "StartLen: 118\n",
      "EndLine: 90\n",
      "StartLen: 108\n",
      "EndLine: 84\n",
      "StartLen: 136\n",
      "EndLine: 100\n",
      "StartLen: 114\n",
      "EndLine: 87\n",
      "StartLen: 408\n",
      "EndLine: 323\n",
      "StartLen: 151\n",
      "EndLine: 118\n",
      "StartLen: 99\n",
      "EndLine: 70\n",
      "StartLen: 385\n",
      "EndLine: 314\n",
      "StartLen: 150\n",
      "EndLine: 117\n",
      "StartLen: 35\n",
      "EndLine: 28\n",
      "StartLen: 212\n",
      "EndLine: 160\n",
      "StartLen: 87\n",
      "EndLine: 68\n",
      "StartLen: 142\n",
      "EndLine: 109\n",
      "StartLen: 445\n",
      "EndLine: 347\n",
      "StartLen: 85\n",
      "EndLine: 63\n",
      "StartLen: 547\n",
      "EndLine: 426\n",
      "StartLen: 81\n",
      "EndLine: 56\n",
      "StartLen: 76\n",
      "EndLine: 59\n",
      "StartLen: 360\n",
      "EndLine: 271\n",
      "StartLen: 36\n",
      "EndLine: 29\n",
      "StartLen: 132\n",
      "EndLine: 104\n",
      "StartLen: 61\n",
      "EndLine: 50\n",
      "StartLen: 120\n",
      "EndLine: 99\n",
      "StartLen: 77\n",
      "EndLine: 60\n",
      "StartLen: 103\n",
      "EndLine: 77\n",
      "StartLen: 302\n",
      "EndLine: 235\n",
      "StartLen: 88\n",
      "EndLine: 68\n",
      "StartLen: 199\n",
      "EndLine: 162\n",
      "StartLen: 492\n",
      "EndLine: 391\n",
      "StartLen: 227\n",
      "EndLine: 182\n",
      "StartLen: 91\n",
      "EndLine: 65\n",
      "StartLen: 38\n",
      "EndLine: 28\n",
      "StartLen: 130\n",
      "EndLine: 103\n",
      "StartLen: 69\n",
      "EndLine: 54\n",
      "StartLen: 95\n",
      "EndLine: 75\n",
      "StartLen: 119\n",
      "EndLine: 90\n",
      "StartLen: 273\n",
      "EndLine: 208\n",
      "StartLen: 95\n",
      "EndLine: 72\n",
      "StartLen: 466\n",
      "EndLine: 391\n",
      "StartLen: 231\n",
      "EndLine: 169\n",
      "StartLen: 207\n",
      "EndLine: 167\n",
      "StartLen: 157\n",
      "EndLine: 117\n",
      "StartLen: 175\n",
      "EndLine: 142\n",
      "StartLen: 498\n",
      "EndLine: 389\n",
      "StartLen: 142\n",
      "EndLine: 115\n",
      "StartLen: 197\n",
      "EndLine: 156\n",
      "StartLen: 77\n",
      "EndLine: 58\n",
      "StartLen: 243\n",
      "EndLine: 190\n",
      "StartLen: 85\n",
      "EndLine: 66\n",
      "StartLen: 449\n",
      "EndLine: 345\n",
      "StartLen: 193\n",
      "EndLine: 152\n",
      "StartLen: 102\n",
      "EndLine: 73\n",
      "StartLen: 74\n",
      "EndLine: 56\n",
      "StartLen: 60\n",
      "EndLine: 47\n",
      "StartLen: 104\n",
      "EndLine: 83\n",
      "StartLen: 104\n",
      "EndLine: 81\n",
      "StartLen: 52\n",
      "EndLine: 38\n",
      "StartLen: 323\n",
      "EndLine: 250\n",
      "StartLen: 200\n",
      "EndLine: 154\n",
      "StartLen: 132\n",
      "EndLine: 103\n",
      "StartLen: 112\n",
      "EndLine: 88\n",
      "StartLen: 37\n",
      "EndLine: 26\n",
      "StartLen: 90\n",
      "EndLine: 65\n",
      "StartLen: 132\n",
      "EndLine: 101\n",
      "StartLen: 432\n",
      "EndLine: 345\n",
      "StartLen: 258\n",
      "EndLine: 199\n",
      "StartLen: 331\n",
      "EndLine: 259\n",
      "StartLen: 281\n",
      "EndLine: 203\n",
      "StartLen: 127\n",
      "EndLine: 95\n",
      "StartLen: 154\n",
      "EndLine: 120\n",
      "StartLen: 106\n",
      "EndLine: 81\n",
      "StartLen: 65\n",
      "EndLine: 44\n",
      "StartLen: 114\n",
      "EndLine: 88\n",
      "StartLen: 103\n",
      "EndLine: 78\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "#for review in xTrain:\n",
    "def removeStopWordsSpace(minValue, maxValue):\n",
    "    counter = minValue\n",
    "\n",
    "    while counter <= maxValue:\n",
    "        print(\"StartLen:\",len(xTrain[counter]))\n",
    "        for word in xTrain[counter]:\n",
    "            if word in stopWordNumbers:\n",
    "                xTrain[counter].remove(word)\n",
    "        print(\"EndLine:\", len(xTrain[counter]))\n",
    "        counter+=1\n",
    "\n",
    "def remvoeStopWords():\n",
    "    startSpace = 0\n",
    "    endSpace = 1000\n",
    "    while endSpace <= 25000:\n",
    "        removeStopWordsSpace(startSpace, endSpace)\n",
    "        startSpace+=1000\n",
    "        endSpace+=1000\n",
    "\n",
    "removeStopWords()\n",
    "\n",
    "\n",
    "#print(\"xTrain: len: \", len(xTrain[123], \", words:\", xTrain[123])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7933da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get properties of the dataset\n",
    "print('Maximum train review length: {}'.format(len(max(xTrain, key=len))))\n",
    "print('Maximum test review length: {}'.format(len(max(xTest, key=len))))\n",
    "print('Minimum train review length: {}'.format(len(min(xTrain, key=len))))\n",
    "print('Minimum test review length: {}'.format(len(min(xTest, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select maximum number of words as input lengt\n",
    "# pad or truncated (this is done automatically) your data\n",
    "maxWords = 1000\n",
    "xTrain = sequence.pad_sequences(xTrain, maxlen=maxWords)\n",
    "xTest = sequence.pad_sequences(xTest, maxlen=maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45999cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the neural network architecture\n",
    "# check out the respective tensorflow help page: https://www.tensorflow.org/guide/keras/rnn\n",
    "model=Sequential()\n",
    "\n",
    "# define size of embedding, see https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Embedding\n",
    "# optional: use a different embedding like word2vec or other options available within tensorflow \n",
    "embeddingSize = 128\n",
    "model.add(Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "# add recurrent layers: \n",
    "# e.g. a SimpleRNN (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/SimpleRNN) with\n",
    "# LayerNormalization (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# add layer for output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print model and check number of parameters\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18535444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for network training\n",
    "batchSize = 64\n",
    "numEpochs = 5\n",
    "\n",
    "# train your model\n",
    "model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "xValid, yValid = xTrain[:batchSize], yTrain[:batchSize]\n",
    "xTrain2, yTrain2 = xTrain[batchSize:], yTrain[batchSize:]\n",
    "hist = model.fit(xTrain2, yTrain2, validation_data=(xValid, yValid), batch_size=batchSize, epochs=numEpochs)\n",
    "\n",
    "# check result\n",
    "scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859de8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
