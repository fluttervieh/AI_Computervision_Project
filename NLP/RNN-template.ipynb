{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6548f38e",
   "metadata": {},
   "source": [
    "### Template for NLP project\n",
    "\n",
    "The aim of the project is to achieve the following:\n",
    " - Train a neural network that is **at least better than random guessing** on your dataset. The template contains the IMDB dataset for sentiment analysis, however, you can choose any other language related data set with the appropriate NLP task.\n",
    " - Investigate different neural network architectures (different hyperparameters, different layers, different pre-processing). Explain in the presentation, why the final network was selected! **Do not rely on black-box mechanisms.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c548b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow modules\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LayerNormalization\n",
    "import tensorflow\n",
    "\n",
    "# if you have installed a different version, replace 'r2.6'  with your version in links provided below\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imdb dataset\n",
    "# links to dataset\n",
    "# original dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# version in tensorflow: https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/imdb\n",
    "\n",
    "# select your vocabulary size\n",
    "vocabularySize = 5000\n",
    "# load data (it is already pre-processed)\n",
    "# optional: add other pre.processing steps like stopword removal\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=vocabularySize)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(xTrain), len(xTest)))\n",
    "\n",
    "# look at the data\n",
    "print('---review---')\n",
    "print(xTrain[123])\n",
    "print('---label---')\n",
    "print(yTrain[123])\n",
    "\n",
    "# look at the respective words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in xTrain[123]])\n",
    "\n",
    "\n",
    "# other related dataset already in tensorflow:  reuters newswire classification dataset\n",
    "# see https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7933da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get properties of the dataset\n",
    "print('Maximum train review length: {}'.format(len(max(xTrain, key=len))))\n",
    "print('Maximum test review length: {}'.format(len(max(xTest, key=len))))\n",
    "print('Minimum train review length: {}'.format(len(min(xTrain, key=len))))\n",
    "print('Minimum test review length: {}'.format(len(min(xTest, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select maximum number of words as input lengt\n",
    "# pad or truncated (this is done automatically) your data\n",
    "maxWords = 1000\n",
    "xTrain = sequence.pad_sequences(xTrain, maxlen=maxWords)\n",
    "xTest = sequence.pad_sequences(xTest, maxlen=maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45999cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the neural network architecture\n",
    "# check out the respective tensorflow help page: https://www.tensorflow.org/guide/keras/rnn\n",
    "model=Sequential()\n",
    "\n",
    "# define size of embedding, see https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Embedding\n",
    "# optional: use a different embedding like word2vec or other options available within tensorflow \n",
    "embeddingSize = 128\n",
    "model.add(Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "# add recurrent layers: \n",
    "# e.g. a SimpleRNN (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/SimpleRNN) with\n",
    "# LayerNormalization (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# add layer for output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print model and check number of parameters\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18535444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for network training\n",
    "batchSize = 64\n",
    "numEpochs = 5\n",
    "\n",
    "# train your model\n",
    "model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "xValid, yValid = xTrain[:batchSize], yTrain[:batchSize]\n",
    "xTrain2, yTrain2 = xTrain[batchSize:], yTrain[batchSize:]\n",
    "hist = model.fit(xTrain2, yTrain2, validation_data=(xValid, yValid), batch_size=batchSize, epochs=numEpochs)\n",
    "\n",
    "# check result\n",
    "scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
