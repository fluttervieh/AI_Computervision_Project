{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6548f38e",
   "metadata": {},
   "source": [
    "### Template for NLP project\n",
    "\n",
    "The aim of the project is to achieve the following:\n",
    " - Train a neural network that is **at least better than random guessing** on your dataset. The template contains the IMDB dataset for sentiment analysis, however, you can choose any other language related data set with the appropriate NLP task.\n",
    " - Investigate different neural network architectures (different hyperparameters, different layers, different pre-processing). Explain in the presentation, why the final network was selected! **Do not rely on black-box mechanisms.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c548b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow modules\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LayerNormalization\n",
    "import tensorflow\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# if you have installed a different version, replace 'r2.6'  with your version in links provided below\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b175c046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n",
      "---review---\n",
      "[1, 307, 5, 1301, 20, 1026, 2511, 87, 2775, 52, 116, 5, 31, 7, 4, 91, 1220, 102, 13, 28, 110, 11, 6, 137, 13, 115, 219, 141, 35, 221, 956, 54, 13, 16, 11, 2714, 61, 322, 423, 12, 38, 76, 59, 1803, 72, 8, 2, 23, 5, 967, 12, 38, 85, 62, 358, 99]\n",
      "[1, 518, 12, 304, 6, 22, 231, 1300, 40, 2, 8, 721, 15, 1727, 117, 142, 15, 955, 2, 5, 2, 2, 15, 2, 6, 87, 20, 42, 6, 87, 229, 83, 6, 991, 31, 18, 4, 2088, 10, 10, 45, 24, 43, 15, 1660, 4669, 65, 47, 195, 3549, 5, 2, 231, 12, 878, 18, 60, 4, 91, 2, 7, 907, 8, 717, 2304, 60, 711, 309, 161, 2396, 38, 78, 45, 89, 2, 2, 4669, 2518, 89, 29, 2, 4, 1511, 83, 268, 58, 15, 2, 4, 3537, 199, 6, 1114, 2, 5, 6, 2, 2, 11, 940, 10, 10, 2, 717, 2, 136, 9, 17, 633, 1307, 4, 20, 4608, 19, 6, 2, 2455, 4764, 1062, 60, 151, 45, 1082, 702, 885, 2699, 1993, 5, 12, 2, 33, 57, 329, 74, 2, 234, 4, 370, 2, 143, 4, 2, 2, 7, 4, 4909, 1455, 40, 12, 9, 49, 243, 7, 2, 2, 2, 18, 4665, 2, 2, 665, 2, 4, 2, 2, 2, 11, 32, 68, 2, 225, 6, 2, 5, 2, 1483, 11, 89, 2, 2, 834, 15, 1791, 72, 55, 76, 7, 89, 860, 907, 952, 11, 4, 4997, 17, 48, 51, 9, 2399, 9, 2, 8, 89, 32, 4, 275, 791, 26, 2, 1004, 4, 2122, 10, 10, 813, 1218, 407, 759, 46, 4, 86, 324, 7, 2, 2, 4736, 125, 268, 39, 4, 2, 7, 6, 1871, 40, 2, 7, 4740, 582, 39, 49, 85, 1873, 42, 1055, 34, 4, 2, 410, 17, 151, 12, 9, 35, 2, 7, 4740, 32, 34, 410, 303, 23, 2, 8, 30, 164, 346, 7, 43, 15, 6, 767, 1872, 2, 143, 4, 2, 2, 11, 550, 1270, 718, 3346, 4, 1312, 19, 4, 2, 4, 370, 728, 367, 19, 90, 2, 4, 206, 17, 12, 571, 305, 7, 3712, 2, 17, 151, 45, 4, 370, 2, 143, 4, 1312, 5, 24, 4, 2, 4, 3282, 702, 2, 11, 4, 2, 393, 2, 178, 1004, 27, 419, 159, 75, 67, 4, 107, 351, 539, 39, 27, 2, 5, 7, 265, 4, 3868, 136, 10, 10, 76, 47, 77, 301, 7, 718, 2, 2, 4792, 27, 1168, 9, 24, 1097, 1112, 88, 73, 240, 718, 4462, 4, 232, 272, 320, 1168, 553, 395, 1168, 505, 90, 83, 35, 3694, 2, 7, 309, 4708, 2, 23, 4, 85, 508, 9, 31, 7, 4, 91, 1581, 973, 2837, 2, 126, 69, 582, 39, 6, 2, 7, 777, 354, 18, 670, 2, 11, 4, 4039, 342, 372, 2, 40, 178, 2, 59, 961, 8, 41, 109, 4, 208, 4655, 7, 2, 5, 921, 2, 6, 1307, 5, 55, 2184, 524]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "['the', 'version', 'to', 'date', 'on', 'list', 'draw', 'him', 'critical', 'very', 'love', 'to', 'by', 'br', 'of', 'its', 'tony', 'characters', 'was', 'one', 'life', 'this', 'is', 'go', 'was', 'best', 'least', 'should', 'so', 'done', 'result', 'no', 'was', 'with', 'this', 'understood', 'only', 'war', \"couldn't\", 'that', 'her', 'get', 'would', 'johnny', 'we', 'in', 'and', 'are', 'to', 'business', 'that', 'her', 'because', 'story', 'use', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# load imdb dataset\n",
    "# links to dataset\n",
    "# original dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# version in tensorflow: https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/imdb\n",
    "\n",
    "# select your vocabulary size\n",
    "vocabularySize = 5000\n",
    "# load data (it is already pre-processed)\n",
    "# optional: add other pre.processing steps like stopword removal\n",
    "(xTrain, yTrain), (xTest, yTest) = imdb.load_data(num_words=vocabularySize)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(xTrain), len(xTest)))\n",
    "\n",
    "# look at the data\n",
    "print('---review---')\n",
    "print(xTrain[123])\n",
    "print(xTrain[124])\n",
    "print('---label---')\n",
    "print(yTrain[123])\n",
    "\n",
    "# look at the respective words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in xTrain[123]])\n",
    "\n",
    "\n",
    "# other related dataset already in tensorflow:  reuters newswire classification dataset\n",
    "# see https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4358fc0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61883a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', \"it's\", 'be', 'hers', 'myself', 'while', 'between', \"weren't\", 'had', 'hadn', \"doesn't\", \"don't\", 'wasn', 'some', 'ourselves', 'about', 'on', 'nor', 'or', 'yourselves', 'through', 'few', \"won't\", 'am', \"hadn't\", \"mightn't\", 'then', 'any', 'down', 'don', 's', 'both', 'above', \"wouldn't\", 'into', 'having', 'yourself', 'from', 'before', 'being', 'm', 'who', 'been', 'herself', \"couldn't\", 'below', 'there', 'over', 'shouldn', 'did', 'can', 'your', 'with', 'mightn', 'where', \"wasn't\", 'out', 'very', 'ma', \"hasn't\", 'shan', 'd', 'needn', 'too', 'because', 'up', 'his', 'couldn', 'all', 'didn', 'hasn', 'whom', 'haven', 'we', 'against', 'further', 'me', 'own', 've', 'should', 'is', \"you're\", 'that', 'off', 'aren', 'and', 'those', 'not', 'during', 'by', 'y', 'as', 'was', \"needn't\", 'only', 'ours', \"she's\", 'an', 'himself', 'here', 'her', 'such', 'has', 'itself', 'of', \"shan't\", 'which', 'you', \"you've\", 'were', \"didn't\", 'wouldn', 'same', 'the', 'a', \"you'd\", 'in', \"you'll\", 'ain', 'now', 'why', \"aren't\", \"isn't\", 'but', 'their', 'our', 'theirs', 'isn', 'it', \"should've\", 'again', 'these', 'each', 'than', 'have', 'most', 'this', 'so', \"shouldn't\", 'once', 'does', 'mustn', 'them', 'what', 'how', 'my', 'll', \"haven't\", 'themselves', 'if', 'yours', 't', 'she', 're', 'its', 'do', 'no', 'will', 'are', \"mustn't\", 'after', 'doesn', 'weren', 'he', 'for', 'under', 'when', 'him', 'at', 'other', 'o', 'to', 'doing', \"that'll\", 'until', 'they', 'just', 'won', 'more'}\n"
     ]
    }
   ],
   "source": [
    "#load Stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14fef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 42, 27, 6139, 543, 134, 197, 1170, 66, 0, 149, 89, 29877, 46, 3144, 41, 20, 882, 39, 9888, 140, 168, 525, 241, 1866, 52778, 92, 98, 177, 1558, 587, 196, 749, 583, 80, 257, 621, 36, 156, 109, 1980, 34, 74, 762, 423, 1905, 47, 117, 0, 119, 67, 126, 16, 0, 118, 283, 43, 52, 8634, 1478, 41501, 1092, 0, 96, 85, 53, 24, 26232, 29, 15496, 41026, 934, 19932, 72, 426, 1034, 69, 202, 13340, 141, 6, 332, 12, 122, 0, 2, 145, 21, 312, 31, 5132, 14, 13, 12421, 61, 11292, 439, 32, 306, 130, 38, 138, 44, 407, 4, 24086, 60, 22, 871, 68, 158, 39964, 169, 1, 3, 1387, 8, 487, 0, 147, 135, 710, 215, 18, 65, 260, 9127, 20781, 9, 5460, 171, 131, 254, 71, 25, 88, 11, 35, 1613, 277, 124, 0, 95, 48, 86, 58, 8725, 771, 530, 45, 6444, 827, 56, 793, 91, 78, 54, 77, 23, 24007, 100, 20830, 0, 26, 15, 464, 51, 87, 30, 82, 1601, 5, 396, 9540, 363, 33, 40, 1196, 50]\n"
     ]
    }
   ],
   "source": [
    "stopWordNumbers = []\n",
    "\n",
    "for word in stopWords:\n",
    "    stopwordId = word2id.get(word, 0)\n",
    "    stopWordNumbers.append(stopwordId)\n",
    "\n",
    "print(stopWordNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eac8a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(xTrain))\n",
    "print(len(xTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75452f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step start: 1000 , End: 2000\n",
      "Step start: 2000 , End: 3000\n",
      "Step start: 3000 , End: 4000\n",
      "Step start: 4000 , End: 5000\n",
      "Step start: 5000 , End: 6000\n",
      "Step start: 6000 , End: 7000\n",
      "Step start: 7000 , End: 8000\n",
      "Step start: 8000 , End: 9000\n",
      "Step start: 9000 , End: 10000\n",
      "Step start: 10000 , End: 11000\n",
      "Step start: 11000 , End: 12000\n",
      "Step start: 12000 , End: 13000\n",
      "Step start: 13000 , End: 14000\n",
      "Step start: 14000 , End: 15000\n",
      "Step start: 15000 , End: 16000\n",
      "Step start: 16000 , End: 17000\n",
      "Step start: 17000 , End: 18000\n",
      "Step start: 18000 , End: 19000\n",
      "Step start: 19000 , End: 20000\n",
      "Step start: 20000 , End: 21000\n",
      "Step start: 21000 , End: 22000\n",
      "Step start: 22000 , End: 23000\n",
      "Step start: 23000 , End: 24000\n",
      "Step start: 24000 , End: 25000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 25000 is out of bounds for axis 0 with size 25000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         endSpace\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep start:\u001b[39m\u001b[38;5;124m\"\u001b[39m, startSpace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, End:\u001b[39m\u001b[38;5;124m\"\u001b[39m, endSpace)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mremvoeStopWords\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mremvoeStopWords\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m endSpace \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m endSpace \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25000\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mremoveStopWordsSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstartSpace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendSpace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     startSpace\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     19\u001b[0m     endSpace\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mremoveStopWordsSpace\u001b[1;34m(minValue, maxValue)\u001b[0m\n\u001b[0;32m      3\u001b[0m counter \u001b[38;5;241m=\u001b[39m minValue\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m maxValue:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#print(\"StartLen:\",len(xTrain[counter]))\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mxTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stopWordNumbers:\n\u001b[0;32m      9\u001b[0m             xTrain[counter]\u001b[38;5;241m.\u001b[39mremove(word)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 25000 is out of bounds for axis 0 with size 25000"
     ]
    }
   ],
   "source": [
    "\n",
    "#for review in xTrain:\n",
    "def removeStopWordsSpace(minValue, maxValue):\n",
    "    removedWords = 0\n",
    "    counter = minValue\n",
    "\n",
    "    while counter <= maxValue:\n",
    "        #print(\"StartLen:\",len(xTrain[counter]))\n",
    "        for word in xTrain[counter]:\n",
    "            if word in stopWordNumbers:\n",
    "                xTrain[counter].remove(word)\n",
    "                removedWords+=1\n",
    "        #print(\"EndLine:\", len(xTrain[counter]))\n",
    "        counter+=1\n",
    "    return removedWords\n",
    "\n",
    "def remvoeStopWords():\n",
    "    print(\"Remove Words\")\n",
    "    startSpace = 0\n",
    "    endSpace = 999\n",
    "    while endSpace <= 25000:\n",
    "        removedWords = removeStopWordsSpace(startSpace, endSpace)\n",
    "        startSpace+=1000\n",
    "        endSpace+=1000\n",
    "        print(\"Step start:\", startSpace, \", End:\", endSpace, \", RemovedWords: \", removedWords)\n",
    "\n",
    "remvoeStopWords()\n",
    "\n",
    "\n",
    "#print(\"xTrain: len: \", len(xTrain[123], \", words:\", xTrain[123])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7933da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get properties of the dataset\n",
    "print('Maximum train review length: {}'.format(len(max(xTrain, key=len))))\n",
    "print('Maximum test review length: {}'.format(len(max(xTest, key=len))))\n",
    "print('Minimum train review length: {}'.format(len(min(xTrain, key=len))))\n",
    "print('Minimum test review length: {}'.format(len(min(xTest, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select maximum number of words as input lengt\n",
    "# pad or truncated (this is done automatically) your data\n",
    "maxWords = 1000\n",
    "xTrain = sequence.pad_sequences(xTrain, maxlen=maxWords)\n",
    "xTest = sequence.pad_sequences(xTest, maxlen=maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45999cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the neural network architecture\n",
    "# check out the respective tensorflow help page: https://www.tensorflow.org/guide/keras/rnn\n",
    "model=Sequential()\n",
    "\n",
    "# define size of embedding, see https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Embedding\n",
    "# optional: use a different embedding like word2vec or other options available within tensorflow \n",
    "embeddingSize = 128\n",
    "model.add(Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "# add recurrent layers: \n",
    "# e.g. a SimpleRNN (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/SimpleRNN) with\n",
    "# LayerNormalization (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# add layer for output\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print model and check number of parameters\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18535444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for network training\n",
    "batchSize = 64\n",
    "numEpochs = 5\n",
    "\n",
    "# train your model\n",
    "model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "xValid, yValid = xTrain[:batchSize], yTrain[:batchSize]\n",
    "xTrain2, yTrain2 = xTrain[batchSize:], yTrain[batchSize:]\n",
    "hist = model.fit(xTrain2, yTrain2, validation_data=(xValid, yValid), batch_size=batchSize, epochs=numEpochs)\n",
    "\n",
    "# check result\n",
    "scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859de8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
